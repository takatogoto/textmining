{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm \n",
    "from mosestokenizer import MosesTokenizer\n",
    "import logging as log\n",
    "from typing import List, Iterator, Set, Dict, Optional, Tuple\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm \n",
    "from mosestokenizer import MosesTokenizer\n",
    "import logging as log\n",
    "\n",
    "log.basicConfig(level=log.INFO)\n",
    "tokr = MosesTokenizer()\n",
    "## split a sentence in string into list whose element is a word\n",
    "\n",
    "def read_tokenized(dir):\n",
    "  \"\"\"Tokenization wrapper\"\"\"\n",
    "  ## call MosesTokenizer for each sentence\n",
    "  inputfile = open(dir)\n",
    "  for sent in inputfile:\n",
    "     yield tokr(sent.strip())\n",
    "\n",
    "## added function\n",
    "def data_proessing(toks):\n",
    "    # toks is list whose element is str\n",
    "    # use list comprehension\n",
    "    # str.lower() change str into lower case letter\n",
    "    # number => <num>, str.isdecimal() check if str is number\n",
    "    out = ['<num>' if s.isdecimal() else s.lower() for s in toks]\n",
    "    # insert <bos> to head of out and <eos> to tail of out\n",
    "    out.insert(0,'<bos>')\n",
    "    out.append('<eos>')\n",
    "    return out  \n",
    "\n",
    "## create train and dev text\n",
    "## call read_tokenized then write it to file \n",
    "train_file = Path('train.txt')\n",
    "with train_file.open('w') as w:\n",
    "  for toks in tqdm(read_tokenized(Path('CSCI-544/hw2/train.txt'))):\n",
    "    #w.write(\" \".join(toks) + '\\n')\n",
    "    w.write(\" \".join(data_proessing(toks)) + '\\n')\n",
    "    \n",
    "dev_file = Path('dev.txt')\n",
    "with dev_file.open('w') as w:\n",
    "  for toks in tqdm(read_tokenized(Path('CSCI-544/hw2/dev.txt'))):\n",
    "    #w.write(\" \".join(toks) + '\\n')\n",
    "    w.write(\" \".join(data_proessing(toks)) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Iterator, Set, Dict, Optional, Tuple\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "## padding and unknow ?\n",
    "RESERVED = ['<pad>', '<unk>']\n",
    "\n",
    "## fixed parameters\n",
    "PAD_IDX = 0 \n",
    "UNK_IDX = 1\n",
    "MAX_TYPES = 10_000\n",
    "BATCH_SIZE = 256\n",
    "MIN_FREQ = 5\n",
    "\n",
    "\n",
    "class Vocab:\n",
    "  \"\"\" Mapper of words <--> index \"\"\"\n",
    "\n",
    "  def __init__(self, types):\n",
    "    # types is list of strings\n",
    "    assert isinstance(types, list)\n",
    "    assert isinstance(types[0], str)\n",
    "\n",
    "    self.idx2word = types\n",
    "    self.word2idx = {word: idx for idx, word in enumerate(types)}\n",
    "    assert len(self.idx2word) == len(self.word2idx)  # One-to-One\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.idx2word)\n",
    "  \n",
    "  def save(self, path: Path):\n",
    "    log.info(f'Saving vocab to {path}')\n",
    "    with path.open('w') as wr:\n",
    "      for word in self.idx2word:\n",
    "        wr.write(f'{word}\\n')\n",
    " \n",
    "  @staticmethod\n",
    "  def load(path):\n",
    "    log.info(f'loading vocab from {path}')\n",
    "    types = [line.strip() for line in path.open()]\n",
    "    for idx, tok in enumerate(RESERVED): # check reserved\n",
    "      assert types[idx] == tok\n",
    "    return Vocab(types)\n",
    "\n",
    "  @staticmethod\n",
    "  def from_text(corpus: Iterator[str], max_types: int,\n",
    "                             min_freq: int = 5):\n",
    "    \"\"\"\n",
    "    corpus: text corpus; iterator of strings\n",
    "    max_types: max size of vocabulary\n",
    "    min_freq: ignore word types that have fewer ferquency than this number\n",
    "    \"\"\"\n",
    "    log.info(\"building vocabulary; this might take some time\")\n",
    "    term_freqs = Counter(tok for line in corpus for tok in line.split())\n",
    "    for r in RESERVED:\n",
    "      if r in term_freqs:\n",
    "        log.warning(f'Found reserved word {r} in corpus')\n",
    "        del term_freqs[r]\n",
    "    term_freqs = list(term_freqs.items())\n",
    "    log.info(f\"Found {len(term_freqs)} types; given max_types={max_types}\")\n",
    "    term_freqs = {(t, f) for t, f in term_freqs if f >= min_freq}\n",
    "    log.info(f\"Found {len(term_freqs)} after dropping freq < {min_freq} terms\")\n",
    "    term_freqs = sorted(term_freqs, key=lambda x: x[1], reverse=True)\n",
    "    term_freqs = term_freqs[:max_types]\n",
    "    types = [t for t, f in term_freqs]\n",
    "    types = RESERVED + types   # prepend reserved words\n",
    "    return Vocab(types)\n",
    "\n",
    "\n",
    "train_file = Path('train.txt')\n",
    "vocab_file = Path('vocab.txt')\n",
    "\n",
    "if not vocab_file.exists():\n",
    "  train_corpus = (line.strip() for line in train_file.open())\n",
    "  vocab = Vocab.from_text(train_corpus, max_types=MAX_TYPES, min_freq=MIN_FREQ)\n",
    "  vocab.save(vocab_file)\n",
    "else:\n",
    "  vocab = Vocab.load(vocab_file)\n",
    "\n",
    "log.info(f'Vocab has {len(vocab)} types')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "class TextDataset:\n",
    "\n",
    "  def __init__(self, vocab: Vocab, path: Path):\n",
    "    self.vocab = vocab\n",
    "    log.info(f'loading data from {path}')\n",
    "    # for simplicity, loading everything to memory; on large datasets this will cause OOM\n",
    "\n",
    "    text = [line.strip().split() for line in path.open()]\n",
    "\n",
    "    # words to index; out-of-vocab words are replaced with UNK\n",
    "    xs = [[self.vocab.word2idx.get(tok, UNK_IDX) for tok in tokss] \n",
    "                 for tokss in text]\n",
    "    \n",
    "    self.data = xs\n",
    "    \n",
    "    log.info(f\"Found {len(self.data)} records in {path}\")\n",
    "\n",
    "  def as_batches(self, batch_size, shuffle=False): # data already shuffled\n",
    "    data = self.data\n",
    "    if shuffle:\n",
    "      random.shuffle(data)\n",
    "    for i in range(0, len(data), batch_size): # i incrememt by batch_size\n",
    "      batch = data[i: i + batch_size]  # slice\n",
    "      yield self.batch_as_tensors(batch)\n",
    "  \n",
    "  @staticmethod\n",
    "  def batch_as_tensors(batch):\n",
    "    \n",
    "    n_ex = len(batch)\n",
    "    max_len = max(len(seq) for seq in batch)\n",
    "    seqs_tensor = torch.full(size=(n_ex, max_len), fill_value=PAD_IDX,\n",
    "                             dtype=torch.long)\n",
    "    \n",
    "    for i, seq in enumerate(batch):\n",
    "      seqs_tensor[i, 0:len(seq)] = torch.tensor(seq)\n",
    "      \n",
    "    return seqs_tensor\n",
    "\n",
    "train_data = TextDataset(vocab=vocab, path=train_file)\n",
    "dev_data = TextDataset(vocab=vocab, path=Path('dev.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class FNN_LM(nn.Module):\n",
    "\n",
    "  def __init__(self, vocab_size, n_class, emb_dim=50, hid=100, dropout=0.2):\n",
    "    super(FNN_LM, self).__init__()\n",
    "    self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                  embedding_dim=emb_dim, \n",
    "                                  padding_idx=PAD_IDX)\n",
    "    self.linear1 = nn.Linear(emb_dim, hid)\n",
    "    self.linear2 = nn.Linear(hid, n_class)\n",
    "    self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "  def forward(self, seqs, log_probs=True):\n",
    "    \"\"\"Return log Probabilities\"\"\"\n",
    "    batch_size, max_len = seqs.shape\n",
    "    embs = self.embedding(seqs)  # embs[Batch x SeqLen x EmbDim]\n",
    "    embs = self.dropout(embs)\n",
    "    embs = embs.sum(dim=1)   # sum over all all steps in seq    \n",
    "    \n",
    "    hid_activated = torch.relu(self.linear1(embs)) # Non linear\n",
    "    scores = self.linear2(hid_activated)\n",
    "\n",
    "    if log_probs:\n",
    "      return torch.log_softmax(scores, dim=1)\n",
    "    else:\n",
    "      return torch.softmax(scores, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_object(model, name):\n",
    "  torch.save({'state_dict': model.state_dict()}, name +\".pt\")\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                    \r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-017b87e6ef5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0mloss_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'none'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m losses = train(model, n_epochs=5, batch_size=BATCH_SIZE, train_data=train_data,\n\u001b[0;32m---> 76\u001b[0;31m                 valid_data=dev_data)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-017b87e6ef5a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, n_epochs, batch_size, train_data, valid_data, device)\u001b[0m\n\u001b[1;32m     28\u001b[0m           \u001b[0;31m#print(\"tasrs shape\", cur_tars.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m           \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_seqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m           \u001b[0;31m#print(\"cur_tars shape\", cur_tars.shape) cur_tars torch.Size([256])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m           \u001b[0;31m#print(\"loss func shape\", log_probs.shape) loss func torch.Size([256, 10002])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/text/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-87cf600e70f5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, seqs, log_probs)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory"
     ]
    }
   ],
   "source": [
    "# Trainer Optimizer \n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "\n",
    "def train(model, n_epochs, batch_size, train_data, valid_data, device=torch.device('cuda')):\n",
    "  log.info(f\"Moving model to {device}\")\n",
    "  model = model.to(device)   # move model to desired device \n",
    "  optimizer = optim.Adam(params=model.parameters())\n",
    "  log.info(f\"Device for training {device}\")\n",
    "  losses = []\n",
    "  for epoch in range(n_epochs):\n",
    "    start = time.time()\n",
    "    num_toks = 0\n",
    "    train_loss = 0.\n",
    "    n_train_batches = 0\n",
    "\n",
    "    model.train() # switch to train mode \n",
    "    with tqdm(train_data.as_batches(batch_size=BATCH_SIZE), leave=False) as data_bar:\n",
    "      for seqs in data_bar:\n",
    "          \n",
    "        seq_loss = torch.zeros(1).to(device)\n",
    "        for i in range(1, seqs.size()[1]-1):\n",
    "          # Move input to desired device\n",
    "          cur_seqs = seqs[:, :i].to(device) # take w0...w_(i-1) python indexing\n",
    "          cur_tars = seqs[:, i].to(device)  # predict w_i\n",
    "          #print(\"seqs shape\", cur_seqs.shape)\n",
    "          #print(\"tasrs shape\", cur_tars.shape)\n",
    "\n",
    "          log_probs = model(cur_seqs)\n",
    "          #print(\"cur_tars shape\", cur_tars.shape) cur_tars torch.Size([256])\n",
    "          #print(\"loss func shape\", log_probs.shape) loss func torch.Size([256, 10002])\n",
    "          seq_loss += loss_func(log_probs, cur_tars).sum() / len(seqs)\n",
    "        \n",
    "        seq_loss /= (seqs.shape[1] - 1) # only n-1 toks are predicted\n",
    "        train_loss += seq_loss.item()\n",
    "        n_train_batches += 1\n",
    "\n",
    "        optimizer.zero_grad()         # clear grads\n",
    "        seq_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pbar_msg = f'Loss:{seq_loss.item():.4f}'\n",
    "        data_bar.set_postfix_str(pbar_msg)\n",
    "\n",
    "    # Run validation\n",
    "    with torch.no_grad():\n",
    "      model.eval() # switch to inference mode -- no grads, dropouts inactive\n",
    "      val_loss = 0\n",
    "      n_val_batches = 0\n",
    "      for seqs in valid_data.as_batches(batch_size=batch_size, shuffle=False):\n",
    "        # Move input to desired device\n",
    "        seq_loss = torch.zeros(1).to(device)\n",
    "        for i in range(1, seqs.size()[1]-1):\n",
    "          # Move input to desired device\n",
    "          cur_seqs = seqs[:, :i].to(device)\n",
    "          cur_tars = seqs[:, i].to(device)\n",
    "\n",
    "          log_probs = model(cur_seqs)\n",
    "          seq_loss += loss_func(log_probs, cur_tars).sum() / len(seqs)\n",
    "        seq_loss /= (seqs.shape[1] - 1)\n",
    "        val_loss += seq_loss.item() \n",
    "        n_val_batches += 1\n",
    "        \n",
    "    save_model_object(model, \"fnn\")  \n",
    "    \n",
    "    avg_train_loss = train_loss / n_train_batches\n",
    "    avg_val_loss = val_loss / n_val_batches\n",
    "    losses.append((epoch, avg_train_loss, avg_val_loss))\n",
    "    log.info(f\"Epoch {epoch} complete; Losses: Train={avg_train_loss:G} Valid={avg_val_loss:G}\")\n",
    "  return losses\n",
    "\n",
    "model = FNN_LM(vocab_size=len(vocab), n_class=len(vocab))\n",
    "loss_func = nn.NLLLoss(reduction='none')\n",
    "losses = train(model, n_epochs=5, batch_size=BATCH_SIZE, train_data=train_data,\n",
    "                valid_data=dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
